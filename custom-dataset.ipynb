{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3945d22e-fb50-486c-b72d-1d84b3d23a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "img_dir = \"images/train/\"\n",
    "\n",
    "csv_file = \"products.csv\"\n",
    "\n",
    "products = pd.read_csv(\"products.csv\")\n",
    "\n",
    "# label list\n",
    "labels_all = list(np.unique(products['GS1 Form'])) + list(np.unique(products['Material'])) + list(np.unique(products['Colour']))\n",
    "label_dict = { i:val for i, val in enumerate(labels_all)}\n",
    "\n",
    "# Define relevant variables for the ML task\n",
    "batch_size = 32\n",
    "num_classes = len(labels_all)\n",
    "learning_rate = 0.01\n",
    "num_epochs = 10\n",
    "\n",
    "# normalizing data ...\n",
    "# transform = transforms.Compose([transforms.Resize((32,32)),\n",
    "#                                      transforms.ToTensor(),\n",
    "#                                      transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "#                                                           std=[0.2023, 0.1994, 0.2010])\n",
    "#                                      ])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Device will determine whether to run the training on GPU or CPU.\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28e91ebe-0a1e-45d9-a1a7-9f01e99eedda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       " \n",
       "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          ...,\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "          [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Loading data with PyTorch\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "import os\n",
    "\n",
    "class MultiLabelDataset(Dataset):\n",
    "    def __init__(self, img_dir, csv_file, transform=None):\n",
    "        self.dataframe = pd.read_csv(csv_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.img_paths = [str(barcode) + '.jpg' for barcode in self.dataframe['Barcode'].values]\n",
    "        self.labels = {'GS1 Form':self.dataframe['GS1 Form'].values, 'Material':self.dataframe['Material'].values, 'Colour':self.dataframe['Colour'].values}\n",
    "        \n",
    "        # dictionary for each class\n",
    "        self.gs1_form = { val:i for i, val in enumerate(list(set(self.labels['GS1 Form'])))}\n",
    "        self.material = { val:i for i, val in enumerate(list(set(self.labels['Material'])))}\n",
    "        self.colour = { val:i for i, val in enumerate(list(set(self.labels['Colour'])))}\n",
    "        \n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_paths[idx])\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "        \n",
    "        # num_classes = len(self.labels_all)\n",
    "        \n",
    "        label1 = torch.nn.functional.one_hot(torch.tensor(self.gs1_form[self.labels['GS1 Form'][idx]]), num_classes = len(self.gs1_form))\n",
    "        label2 = torch.nn.functional.one_hot(torch.tensor(self.material[self.labels['Material'][idx]]), num_classes = len(self.material))\n",
    "        label3 = torch.nn.functional.one_hot(torch.tensor(self.colour[self.labels['Colour'][idx]]), num_classes = len(self.colour))\n",
    "        \n",
    "        label = torch.cat((label1, label2, label3))\n",
    "        \n",
    "        # label = torch.nn.functional.one_hot(label, num_classes = num_classes)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img, label\n",
    "\n",
    "class MultiLabelDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size=batch_size, shuffle=False, num_workers=0):\n",
    "        super(MultiLabelDataLoader, self).__init__(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for i, (data, label) in enumerate(super().__iter__()):\n",
    "            yield (data, label)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)  \n",
    "\n",
    "# Create an instance of the custom dataset class\n",
    "train_dataset = MultiLabelDataset(img_dir, csv_file, transform=transform)\n",
    "\n",
    "# Create an instance of the custom data loader class\n",
    "# train_loader = MultiLabelDataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# data_iter = iter(train_loader)\n",
    "# images, labels = next(data_iter)\n",
    "\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a97c91da-cedd-483a-8f45-75511168843d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(images[0].permute(1, 2, 0))\n",
    "\n",
    "# val, indexes = torch.max(labels[0], dim=1)\n",
    "\n",
    "# print(train_dataset.R_gs1_form[int(indexes[0])])\n",
    "# print(train_dataset.R_material[int(indexes[1])])\n",
    "# print(train_dataset.R_colour[int(indexes[2])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33bba27-e262-449f-b0ff-3da38f87bef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845e7c378a2b4d6daccfcb24032ef486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354c6c6a96b741b6b6b0e0f476822e6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/10], Loss: 0.6989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a7d7256a0f4e1c8ee5548a8848e30e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/10], Loss: 0.6751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4796e671bb4590925d5c8f15522753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/10], Loss: 0.6751\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6690da2dae76485e8cd70ff4cdddf312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/10], Loss: 0.6989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d5414ed9f05414a904ee0404bdb8d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Loss: 0.6989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3cbaea32ec41deaae90a67e5799b4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/10], Loss: 0.6989\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "183a4dd0316a406289f7fed0b4887563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8:   0%|          | 0/313 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a CNN model\n",
    "class MultiLabelCNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLabelCNN, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(4096, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 21)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = torch.nn.functional.relu(self.fc1(x))\n",
    "        x = torch.nn.functional.sigmoid(self.fc2(x))\n",
    "        return x\n",
    "\n",
    "# Initialize model, loss function and optimizer\n",
    "model = MultiLabelCNN()\n",
    "criterion = torch.nn.MultiLabelSoftMarginLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader, desc=(f'Epoch {epoch + 1}'))):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        # labels = labels.view(-1, 21)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "# Test the model\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in test_loader:\n",
    "#         outputs = model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "#     print('Test Accuracy of the model on the test images: {} %'.format(100 * correct / total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
